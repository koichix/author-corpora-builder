menu.vysledky=RESULTS
menu.autor=AUTHOR
menu.nastavenia=SETTINGS
menu.info=INFO
homepage.input=Enter new url or choose from examples below
homepage.input.info=Place to input url adress.
homepage.back=Back to home page
homepage.run=Download and analyse web page.
author.is=IS profile
author.linkedin=LinkedIn profile
author.email=Send email
info.description.header=Description
info.description1=Software for automatic extraction of information from web articles.
info.description2=Software is used for automatic detection of main content from internet articles. After entering url address, it downloads a selectet amount of articles (can be changed in settings), and extracts heading, author, date and the main content. Output is a ZIP file, which contains XML files created based on author's name and it contains his articles.
info.instruction.header=How to use it
info.instruction.run=Running the software
info.instruction.settings=Settings
info.instruction.output=Results
info.instruction.run2=software downloads and analyses pages within selected URL address from input field. Afterwards it analyses data and the result is written to a table. Results can be downloaded one by one or as a zip file.
info.instruction.settings2=number of pages to download, crawl depth, politeness and other settings.
info.instruction.output2=example data which can be viewed online or downloaded as a zip file.
preview.info=The application was tested on 20 Czech and Slovak web sites, from each we extracted 500 articles. The average success rate of our algorithm is 90,68%  Our application has shown that it can be effciently used to build corpora for stylometry research. The corpus along with full text of article and reference to web source contains information about author, document title and creation date.
preview.graph=Autorship corpora built from 20 domains
preview.download=Application can be downloaded -
preview.download2=Built authorship corpora -
preview.manager=File manager
preview.file=Actual XML file
output=Output
output.info=(list of articles with extracted data)
output.original=Original article
output.extracted=Extracted data
output.xml=The table shows original article alongside with extracted data, which can be downloaded separately as xml files.
output.download=All files together can be also downloaded:
settings.heading=Settings - description:
settings.number=Number of pages
settings.wait=Politeness
settings.url=Custom URL
settings.thresh=Threshold
settings.depth=Crawling depth
settings.number2=sets number of pages to download.
settings.wait2=sets politenesss - time to wait between downloading multiple pages in miliseconds.
settings.url2=sets filter of URL for downloading pages.
settings.thresh2=sets threshold for cleaning boilerplate with SST. 
settings.depth2=integer from 1 to N stands for depth within selected URL. For example: on page "kultura.sme.sk/clanok1" is crawl depth 1 part of the url - "sme.sk". Crawl depth -1 stands for infinite depth.
settings.url.on=Enables custom URL.
settings.url.on1=Enable
settings.save=Save settings
settings.number3=Number of pages to download [integer].
settings.wait3=Wait time between downloading multipla pages [miliseconds].
settings.url3=Custom URL adress [URL adress].
settings.thresh3=Threshold for SST [decimal betweeni 0 and 1].
settings.depth3=Crawling depth [integer].
